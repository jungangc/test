{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Bu5mKQbMEQ_"
   },
   "source": [
    "## Project 2       Jungang (Gordon) Chen            UIN: 5280097929"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBU2xsA8MERA"
   },
   "source": [
    "## 1)  Load the text dataset and text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CNht1omtMERB"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elEoWObFMERC",
    "outputId": "e4d3d4ba-3bef-4f9e-87e4-d12669d5b418",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c d a e d a e e b g a f f g b f c g b d a k j k c e a i i l m a d h ed b e a e ef c e a d eg eh a g ee ei a e ej \n"
     ]
    }
   ],
   "source": [
    "input_sequence = pickle.load(open('DS_5_train_input', 'rb'))\n",
    "output_sequence = pickle.load(open('DS_5_train_output', 'rb'))\n",
    "print(output_sequence[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-MOaaHMMERD",
    "outputId": "b761dde3-923e-4e38-ff01-feb75a6bcbb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a j b f a f c d a k c e a f b d b g a f c g a d b f a k c g c d ', '[start] b f c d c e b d b g a f g h a k f i a f e j c g b f c g c d a k ed ee a d m ef a f l eg a j d k eh [end]')\n"
     ]
    }
   ],
   "source": [
    "text_pairs = []\n",
    "for line in range(len(input_sequence)):\n",
    "    inputish  = input_sequence[line]\n",
    "    outputish = \"[start] \" + output_sequence[line] + \"[end]\"\n",
    "    text_pairs.append((inputish, outputish))\n",
    "\n",
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DMgmvon4eKXH",
    "outputId": "c346f561-6e07-4611-ca4a-7eec7d147d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "print(len(text_pairs))\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TImKx8WBJiMS",
    "outputId": "7c5c7693-8c36-4ed0-f149-a7bd07034785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "a d c g a d a h a g a e c d a f a h b f c e a k b e a e a j a k a e c d b g b e b f a g a d b d b f b g b d b g b e \n",
      "13\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "src_vocab_size = 50\n",
    "tgt_vocab_size = 50\n",
    "sequence_length = 100\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=src_vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=tgt_vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length+1, \n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "    # standardize=custom_standardization,\n",
    "train_inputish_texts = [pair[0] for pair in train_pairs]\n",
    "print(type(train_inputish_texts))\n",
    "print(train_inputish_texts[0])\n",
    "train_outputish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_inputish_texts)\n",
    "target_vectorization.adapt(train_outputish_texts)\n",
    "print(len(source_vectorization.get_vocabulary()))\n",
    "\n",
    "print(len(target_vectorization.get_vocabulary()))\n",
    "\n",
    "# save the text vectorization for future testing\n",
    "pickle.dump({'config': source_vectorization.get_config(),\n",
    "             'weights': source_vectorization.get_weights()}\n",
    "            , open(\"source_tv_layer.pkl\", \"wb\"))\n",
    "\n",
    "pickle.dump({'config': target_vectorization.get_config(),\n",
    "             'weights': target_vectorization.get_weights()}\n",
    "            , open(\"target_tv_layer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nR11xxwKJiMf",
    "outputId": "a6d63280-057f-485b-ad39-653b42d2ca13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CacheDataset element_spec=({'english': TensorSpec(shape=(None, 100), dtype=tf.int64, name=None), 'spanish': TensorSpec(shape=(None, 100), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],\n",
    "    }, spa[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)      # unzip the text pairs\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1qZktTgJiMw",
    "outputId": "9fc24b30-85bc-436c-c322-8056250a3103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (16, 100)\n",
      "inputs['spanish'].shape: (16, 100)\n",
      "targets.shape: (16, 100)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFiO_M1FJiMx"
   },
   "source": [
    "## 2) Build a transformer model with a stack of 2 encoder and 3 decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "6Ni-1zLOJiMx"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "fCbbkJhpNJ3_"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "bSd4miDzJiMz"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "UmBJWX24WxCs"
   },
   "outputs": [],
   "source": [
    "embed_dim = 64\n",
    "dense_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, src_vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, tgt_vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(tgt_vocab_size, activation=\"softmax\")(x)\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzjZ5hlGJiM0",
    "outputId": "4d83247a-05fc-46d3-8fda-3f712cbb883f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - 17s 42ms/step - loss: 1.3977 - accuracy: 0.3003 - val_loss: 0.9445 - val_accuracy: 0.4171\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.9582 - accuracy: 0.4283 - val_loss: 0.8373 - val_accuracy: 0.4801\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.8140 - accuracy: 0.4949 - val_loss: 0.7469 - val_accuracy: 0.5327\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.7233 - accuracy: 0.5442 - val_loss: 0.6144 - val_accuracy: 0.5941\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.6597 - accuracy: 0.5772 - val_loss: 0.5778 - val_accuracy: 0.6152\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.6219 - accuracy: 0.6008 - val_loss: 0.5233 - val_accuracy: 0.6432\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.5846 - accuracy: 0.6221 - val_loss: 0.5132 - val_accuracy: 0.6495\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.5465 - accuracy: 0.6494 - val_loss: 0.4797 - val_accuracy: 0.6768\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.5083 - accuracy: 0.6779 - val_loss: 0.4754 - val_accuracy: 0.6872\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 9s 40ms/step - loss: 0.4725 - accuracy: 0.7002 - val_loss: 0.4058 - val_accuracy: 0.7327\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.4436 - accuracy: 0.7199 - val_loss: 0.4150 - val_accuracy: 0.7244\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.4121 - accuracy: 0.7393 - val_loss: 0.3696 - val_accuracy: 0.7570\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.3836 - accuracy: 0.7593 - val_loss: 0.3631 - val_accuracy: 0.7623\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.3584 - accuracy: 0.7772 - val_loss: 0.3367 - val_accuracy: 0.7834\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.3363 - accuracy: 0.7914 - val_loss: 0.2999 - val_accuracy: 0.8046\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.3111 - accuracy: 0.8076 - val_loss: 0.2664 - val_accuracy: 0.8282\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.2876 - accuracy: 0.8234 - val_loss: 0.3694 - val_accuracy: 0.7838\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.2698 - accuracy: 0.8364 - val_loss: 0.2358 - val_accuracy: 0.8517\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.2531 - accuracy: 0.8495 - val_loss: 0.2082 - val_accuracy: 0.8690\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.2338 - accuracy: 0.8602 - val_loss: 0.2317 - val_accuracy: 0.8540\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.2201 - accuracy: 0.8697 - val_loss: 0.1913 - val_accuracy: 0.8813\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.2081 - accuracy: 0.8777 - val_loss: 0.1830 - val_accuracy: 0.8894\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.1955 - accuracy: 0.8865 - val_loss: 0.1918 - val_accuracy: 0.8846\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.1849 - accuracy: 0.8937 - val_loss: 0.1586 - val_accuracy: 0.9026\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.1729 - accuracy: 0.9007 - val_loss: 0.1725 - val_accuracy: 0.8981\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.1654 - accuracy: 0.9062 - val_loss: 0.1658 - val_accuracy: 0.9010\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.1555 - accuracy: 0.9119 - val_loss: 0.2615 - val_accuracy: 0.8528\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.1493 - accuracy: 0.9166 - val_loss: 0.1366 - val_accuracy: 0.9188\n",
      "Epoch 29/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.1420 - accuracy: 0.9218 - val_loss: 0.1799 - val_accuracy: 0.8920\n",
      "Epoch 30/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.1322 - accuracy: 0.9270 - val_loss: 0.1126 - val_accuracy: 0.9331\n",
      "Epoch 31/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.1302 - accuracy: 0.9295 - val_loss: 0.1273 - val_accuracy: 0.9267\n",
      "Epoch 32/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.1207 - accuracy: 0.9344 - val_loss: 0.1124 - val_accuracy: 0.9339\n",
      "Epoch 33/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.1168 - accuracy: 0.9369 - val_loss: 0.1302 - val_accuracy: 0.9260\n",
      "Epoch 34/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.1112 - accuracy: 0.9406 - val_loss: 0.1519 - val_accuracy: 0.9153\n",
      "Epoch 35/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.1044 - accuracy: 0.9442 - val_loss: 0.1182 - val_accuracy: 0.9341\n",
      "Epoch 36/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0999 - accuracy: 0.9467 - val_loss: 0.1088 - val_accuracy: 0.9385\n",
      "Epoch 37/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0972 - accuracy: 0.9485 - val_loss: 0.0930 - val_accuracy: 0.9477\n",
      "Epoch 38/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0931 - accuracy: 0.9513 - val_loss: 0.1569 - val_accuracy: 0.9198\n",
      "Epoch 39/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0898 - accuracy: 0.9532 - val_loss: 0.1109 - val_accuracy: 0.9378\n",
      "Epoch 40/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0860 - accuracy: 0.9551 - val_loss: 0.1465 - val_accuracy: 0.9233\n",
      "Epoch 41/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0828 - accuracy: 0.9568 - val_loss: 0.1025 - val_accuracy: 0.9406\n",
      "Epoch 42/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0780 - accuracy: 0.9598 - val_loss: 0.0937 - val_accuracy: 0.9509\n",
      "Epoch 43/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0760 - accuracy: 0.9609 - val_loss: 0.0795 - val_accuracy: 0.9565\n",
      "Epoch 44/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0754 - accuracy: 0.9615 - val_loss: 0.1096 - val_accuracy: 0.9414\n",
      "Epoch 45/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0709 - accuracy: 0.9646 - val_loss: 0.1204 - val_accuracy: 0.9325\n",
      "Epoch 46/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0687 - accuracy: 0.9652 - val_loss: 0.1077 - val_accuracy: 0.9447\n",
      "Epoch 47/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0663 - accuracy: 0.9667 - val_loss: 0.0901 - val_accuracy: 0.9518\n",
      "Epoch 48/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0675 - accuracy: 0.9663 - val_loss: 0.0675 - val_accuracy: 0.9625\n",
      "Epoch 49/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0618 - accuracy: 0.9690 - val_loss: 0.0778 - val_accuracy: 0.9592\n",
      "Epoch 50/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0612 - accuracy: 0.9695 - val_loss: 0.1016 - val_accuracy: 0.9497\n",
      "Epoch 51/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0601 - accuracy: 0.9704 - val_loss: 0.0723 - val_accuracy: 0.9626\n",
      "Epoch 52/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0583 - accuracy: 0.9707 - val_loss: 0.1055 - val_accuracy: 0.9495\n",
      "Epoch 53/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0555 - accuracy: 0.9724 - val_loss: 0.0751 - val_accuracy: 0.9608\n",
      "Epoch 54/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0552 - accuracy: 0.9724 - val_loss: 0.0753 - val_accuracy: 0.9620\n",
      "Epoch 55/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0546 - accuracy: 0.9730 - val_loss: 0.0654 - val_accuracy: 0.9661\n",
      "Epoch 56/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0516 - accuracy: 0.9749 - val_loss: 0.0787 - val_accuracy: 0.9602\n",
      "Epoch 57/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0512 - accuracy: 0.9751 - val_loss: 0.0667 - val_accuracy: 0.9657\n",
      "Epoch 58/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0491 - accuracy: 0.9761 - val_loss: 0.0661 - val_accuracy: 0.9670\n",
      "Epoch 59/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0498 - accuracy: 0.9759 - val_loss: 0.0661 - val_accuracy: 0.9674\n",
      "Epoch 60/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0470 - accuracy: 0.9776 - val_loss: 0.0695 - val_accuracy: 0.9660\n",
      "Epoch 61/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0455 - accuracy: 0.9777 - val_loss: 0.0680 - val_accuracy: 0.9649\n",
      "Epoch 62/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0458 - accuracy: 0.9784 - val_loss: 0.0807 - val_accuracy: 0.9633\n",
      "Epoch 63/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0449 - accuracy: 0.9788 - val_loss: 0.0633 - val_accuracy: 0.9673\n",
      "Epoch 64/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0416 - accuracy: 0.9800 - val_loss: 0.0660 - val_accuracy: 0.9683\n",
      "Epoch 65/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0430 - accuracy: 0.9796 - val_loss: 0.1070 - val_accuracy: 0.9567\n",
      "Epoch 66/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0410 - accuracy: 0.9802 - val_loss: 0.0713 - val_accuracy: 0.9649\n",
      "Epoch 67/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0431 - accuracy: 0.9799 - val_loss: 0.0874 - val_accuracy: 0.9560\n",
      "Epoch 68/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0393 - accuracy: 0.9816 - val_loss: 0.0584 - val_accuracy: 0.9706\n",
      "Epoch 69/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0411 - accuracy: 0.9805 - val_loss: 0.0559 - val_accuracy: 0.9730\n",
      "Epoch 70/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0375 - accuracy: 0.9824 - val_loss: 0.0723 - val_accuracy: 0.9668\n",
      "Epoch 71/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0374 - accuracy: 0.9824 - val_loss: 0.0699 - val_accuracy: 0.9679\n",
      "Epoch 72/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0375 - accuracy: 0.9828 - val_loss: 0.0734 - val_accuracy: 0.9646\n",
      "Epoch 73/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0360 - accuracy: 0.9833 - val_loss: 0.0585 - val_accuracy: 0.9721\n",
      "Epoch 74/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0370 - accuracy: 0.9829 - val_loss: 0.0599 - val_accuracy: 0.9706\n",
      "Epoch 75/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0349 - accuracy: 0.9837 - val_loss: 0.0642 - val_accuracy: 0.9709\n",
      "Epoch 76/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0358 - accuracy: 0.9833 - val_loss: 0.0699 - val_accuracy: 0.9652\n",
      "Epoch 77/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0324 - accuracy: 0.9844 - val_loss: 0.0640 - val_accuracy: 0.9695\n",
      "Epoch 78/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0344 - accuracy: 0.9839 - val_loss: 0.0597 - val_accuracy: 0.9696\n",
      "Epoch 79/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0323 - accuracy: 0.9850 - val_loss: 0.0518 - val_accuracy: 0.9758\n",
      "Epoch 80/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0320 - accuracy: 0.9853 - val_loss: 0.0867 - val_accuracy: 0.9651\n",
      "Epoch 81/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0322 - accuracy: 0.9853 - val_loss: 0.0541 - val_accuracy: 0.9756\n",
      "Epoch 82/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0291 - accuracy: 0.9863 - val_loss: 0.0670 - val_accuracy: 0.9707\n",
      "Epoch 83/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0307 - accuracy: 0.9859 - val_loss: 0.0507 - val_accuracy: 0.9765\n",
      "Epoch 84/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0309 - accuracy: 0.9859 - val_loss: 0.0668 - val_accuracy: 0.9694\n",
      "Epoch 85/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0298 - accuracy: 0.9864 - val_loss: 0.0623 - val_accuracy: 0.9712\n",
      "Epoch 86/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0302 - accuracy: 0.9861 - val_loss: 0.0552 - val_accuracy: 0.9730\n",
      "Epoch 87/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0265 - accuracy: 0.9874 - val_loss: 0.0525 - val_accuracy: 0.9764\n",
      "Epoch 88/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0302 - accuracy: 0.9864 - val_loss: 0.0484 - val_accuracy: 0.9763\n",
      "Epoch 89/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0276 - accuracy: 0.9876 - val_loss: 0.0536 - val_accuracy: 0.9750\n",
      "Epoch 90/100\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.0266 - accuracy: 0.9874 - val_loss: 0.0480 - val_accuracy: 0.9774\n",
      "Epoch 91/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0289 - accuracy: 0.9875 - val_loss: 0.0612 - val_accuracy: 0.9727\n",
      "Epoch 92/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0280 - accuracy: 0.9873 - val_loss: 0.0677 - val_accuracy: 0.9704\n",
      "Epoch 93/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0262 - accuracy: 0.9880 - val_loss: 0.0563 - val_accuracy: 0.9738\n",
      "Epoch 94/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0250 - accuracy: 0.9887 - val_loss: 0.0653 - val_accuracy: 0.9719\n",
      "Epoch 95/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0273 - accuracy: 0.9878 - val_loss: 0.0565 - val_accuracy: 0.9756\n",
      "Epoch 96/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0258 - accuracy: 0.9885 - val_loss: 0.0524 - val_accuracy: 0.9769\n",
      "Epoch 97/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0254 - accuracy: 0.9889 - val_loss: 0.0739 - val_accuracy: 0.9694\n",
      "Epoch 98/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0245 - accuracy: 0.9889 - val_loss: 0.0821 - val_accuracy: 0.9669\n",
      "Epoch 99/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0266 - accuracy: 0.9883 - val_loss: 0.0615 - val_accuracy: 0.9704\n",
      "Epoch 100/100\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0230 - accuracy: 0.9895 - val_loss: 0.0509 - val_accuracy: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0c6ec7a6d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"artificial_text_translation.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "transformer.fit(train_ds, epochs=100, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhS2m_Trjqc7"
   },
   "source": [
    "## 3) Model Accuracy by randomly selecting 50 true/prediction testing sentences and compare them token by token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOUOjN5rV8VA",
    "outputId": "359a0404-dba9-416b-8432-6faf3f8ba705",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a g a k a d c e a d b f a k b e a j a g a f b d c e b f b e c d c g b e \n",
      "[start] c e b f b e b d c e a f g h b f a g i j b e c d a j k l m a k f ed a d e ee a d d ef c g a k eg eh b e a g ei ej [end]\n",
      "[start] c e b f b e b d c e a f g h b f a g i j b e c d a j k l m a k f ed a d e ee a d d ef c g a k eg eh b e a g ei ej [end]\n",
      "a h a d b g a d a i b d b d a k b e a e a e a h a k c e b d c e b d c e c d c e \n",
      "[start] b g b d b d b e c e b d a k h i c e b d a h j k l a e m a e ed a k g ee a i e f ef c e a d eg eh a d d ei c d c e a h ej ek el [end]\n",
      "[start] b g b d b d b e c e b d a k h i c e b d a h j k l a e m a k g ed a i e f ee c e a d ef eg a d d eh c d c e a h ei ej ek [end]\n",
      "a f a f a d a d a d c f b d b g a d b d c d c g a d a g a i a g c g c d c f a e c d b d b g \n",
      "[start] c f b d a d d e b g a d f g b d c d a d i j a d h k c g a f l m c g c d a g ee ef c f c d a e ei a i eg eh ej b d a g ek el b g a d em fd a f ed fe [end]\n",
      "[start] c f b d a d d e b g a d f g b d c d a d i j a d h k c g a f l m c g c d a g ee ef c f c d a e ei a i eg eh ej b d a g ek el b g a d em fd a f ed fe [end]\n",
      "a h a h b f a f b d a e a k a g b f a d a j c d a i b e c f a e a d a g c g b e c d c f b e c g c e b g c g \n",
      "[start] b f b d b f c d b e c f c g b e a g j k c d a d l m a e ed a i h i ee c f a j g ef eg b e a d eh ei a g f ej c g a k ek el a e em a f e fd c e a h d fe ff b g c g a h fg fh fi [end]\n",
      "[start] b f b d b f c d b e c f c g a e j b e a g k l c d a d m ed a i h i ee c f a j g ef eg c e a d eh ei a g f ej c g a k ek el a e em a f e fd c e a h d fe ff b g c g a h fg fh fi [end]\n",
      "a e a h a g c d a e c f a i b d a h c e c f a j b g c e b d c g a h c f a j a j c d c f c f c e b f c g \n",
      "[start] c d c f a e e a g d f b d c e c f b g c e b d a j k l m a h i j ed c g a i h ee ef c f c d c f c f a j ei ej ek c e b f a j el em fd c g a h eh fe ff a h g eg fg a e fh [end]\n",
      "[start] c d c f a e e a g d f b d c e c f b g c e b d a j k l m a h i j ed c g a i h ee ef c f c f c f c f a j ei ej ek c e b f a j el em fd a h eg eh fe c g a h g ff fg a e fh [end]\n",
      "a d b f a k b d a h c g a f a f a e a h c e a k c e b e a k b f a e b e c f c g a k b d b g \n",
      "[start] b f b d c g c e c e b e a k h i b f b e a e l a k k m a h g j ed a e ee c f a f ef eg c g a f eh ei b d b g a k ek el a h f ej em a k e fd a d d fe [end]\n",
      "[start] b f b d c g c e c e b e a k h i b f b e c e a e m a k l ed a k j ee a h g k ef c g a f eg eh b d b g a k ej ek a f ei el a h e f em a d d fd [end]\n",
      "a f a f a d a d a d c f b d b g a d b d c d c g a d a g a i a g c g c d c f a e c d b d b g \n",
      "[start] c f b d a d d e b g a d f g b d c d a d i j a d h k c g a f l m c g c d a g ee ef c f c d a e ei a i eg eh ej b d a g ek el b g a d em fd a f ed fe [end]\n",
      "[start] c f b d a d d e b g a d f g b d c d a d i j a d h k c g a f l m c g c d a g ee ef c f c d a e ei a i eg eh ej b d a g ek el b g a d em fd a f ed fe [end]\n",
      "a i b d a g a e a h b f a j a j a e a e a e b e c f b d c f c e a d b f b g a e a i a h b g b f a d a g c d c g c g c f c g b d \n",
      "[start] b d b f b e a e f a e g a e h c f b d a j i j k c f c e a j l m ed b f b g a d ef eg a h e ee eh a e ei b g b f c d c g a g em fd c g a d fe ff a h ek el fg c f c g a i fh fi fj a e fk a g ej fl b d a i d fm gd [end]\n",
      "[start] b d b f a e e b e a e g a e h c f b d a j i j k c f c f a j l m ed b f b g a d ef eg a e eh a h f ee ei a d d ej b f b g c d c g a g fd fe c g a d ff fg a d ff fh a e fi c f b g a i fj fk fl a i ek fl fm [end]\n",
      "a d a i a e a d b d c g b g c e a d a h c d a f b d b e b d a f b g b d \n",
      "[start] b d c g a d d e a e f b g c e a i g h i c d b d b e a f l m b d a h k ed ee b g b d a f eg eh a d ef ei a d j ej [end]\n",
      "[start] b d c g a d d e a e f b g c e a i g h i c d b d b e a f l m b d a h k ed ee b g b d a f eg eh a d ef ei a d j ej [end]\n",
      "a h a g b f b e a f c g a k a d a k a g c g a d c d b f a e c g a f c e c g a f b d b e c f \n",
      "[start] b f b e a g d e c g c g c d b f a d i j a g h k c g a e m a k l ed c e c g a f ef eg a d ee eh b d b e a f ej ek a k ei el a f g em c f a h f fd fe [end]\n",
      "[start] b f b e a g d e c g c g c d b f a d i j a g h k c g a e m a k l ed c e c g a f ef eg a d ee eh b d b e a f ej ek a k ei el a f g em c f a h f fd fe [end]\n",
      "a j b d c e a d a f c g a j b e c d a g b f a h a k a e b d b d b e c f a h b g a f c f a h c d a f c g c d b g b e \n",
      "[start] b d c e c g b e c d b f b d a e j b d a k k l b e c f a h m ed ee a g i ef a j g h eg a f f eh b g c f c d c g c d a f em fd b g a h el fe ff a f ek fg b e a h ej fh fi a d ei fj a j d e fk [end]\n",
      "[start] b d c e c g b e c d b f b d a e j b d a k k l b e a k m ed a g i ee a j g h ef a f f eg c f b g c f c d c g a f el em b d a h ek fd fe a f ej ff b e a h ei fg fh a d eh fi a j d e fj [end]\n",
      "a h c f a e b g a g a d c e a k b e b d a k a j a d a e a k a k c e c e c g c f b g a g c g c g b f \n",
      "[start] c f b g a e e c e b e b d a k h i a d g j c e c e a k l m c g a k ed ee a e ef c f a d eg eh b g c g c g a g ek el a j ei ej em b f a k fd fe a g k ff a h d f fg [end]\n",
      "[start] c f b g a e e c e b e b d a k h i a d g j c e c e c g a k m ed a k l ee a e ef c f a d eg eh b g c g c g a g ek el a j ei ej em b f a k fd fe a g k ff a h d f fg [end]\n",
      "a j c g a d b g a k a j b d b f a g a g a e c d b e a e a d b d a k b d a i b f a j c g b g c e b e a f c d b e a d c g b d \n",
      "[start] c g b g b d b f c d a e h b e a g i j b d b d b f c g b g c e a j ee ef eg b e a i ed eh ei a k m ej a d l ek a e el a g k em a j f g fd c d b e a f ff fg a k fe fh a d e fi c g b d a d fk fl a j d fj fm [end]\n",
      "[start] c g b g b d b f c d a e h b e a g i j b d b d b f c g b e a i ed ee ef c f b e a h eg eh ei a k m ej a d l ek a e el a g k em c d b g a d fe ff a g fd fg a j f g fh c d b d a d fj fk a k fi fl a j d e fm [end]\n",
      "a g a h a i a e b d a i c d a e c g b e b g a i c e b d c g b f a e c d \n",
      "[start] b d a e d c d c g a e g b e a i f h i b g a i e j k c e b d c g a i m ed ee b f a h l ef eg c d a e ei a g eh ej [end]\n",
      "[start] b d a e d c d c g a e g b e a i f h i b g a i e j k c e b d c g a i m ed ee b f a h l ef eg c d a e ei a g eh ej [end]\n",
      "a h a g b f b e a f c g a k a d a k a g c g a d c d b f a e c g a f c e c g a f b d b e c f \n",
      "[start] b f b e a g d e c g c g c d b f a d i j a g h k c g a e m a k l ed c e c g a f ef eg a d ee eh b d b e a f ej ek a k ei el a f g em c f a h f fd fe [end]\n",
      "[start] b f b e a g d e c g c g c d b f a d i j a g h k c g a e m a k l ed c e c g a f ef eg a d ee eh b d b e a f ej ek a k ei el a f g em c f a h f fd fe [end]\n",
      "a h a d a h c f a e b e a j a f c d c g a f b g a k b g c g a j b d a e b e c d b g b e c g \n",
      "[start] c f b e a e e c d c g a f g h b g b g c g a k k l a f j m b d b e a e ef c d a j ee eg eh a j i ed ei a h d f ej b g a d ek el b e c g a h em fd fe [end]\n",
      "[start] c f b e a e e c d c g a f g h b g b g c g a k k l a f j m b d b e a e ef c d a j ee eg eh a j i ed ei a h d f ej b g a d ek el b e c g a h em fd fe [end]\n",
      "a j a k a g a j a g b g b g a i c f b f c e c f a e b d b d b d b e \n",
      "[start] b g b g a g d e c f b f c e a i g h i c f a j f j k b d a e m a g l ed b d a k ee ef b d b e a j eg eh ei [end]\n",
      "[start] b g b g a g d e c f b f c e a i g h i c f a j f j k b d a e m a g l ed b d a k ee ef b d b e a j eg eh ei [end]\n",
      "a j a j a d a d b e c g c f b d b g a e a f b d c d a k c e b g \n",
      "[start] b e c g a d d e c f a d f g b d b g a j h i j b d c d a f l m a e ed c e b g a k ef eg a j k ee eh [end]\n",
      "[start] b e c g a d d e c f a d f g b d b g a j h i j b d c d a f l m a e ed c e b g a k ef eg a j k ee eh [end]\n",
      "a f c d a g a k c e a f a f a f a f a i a d b g b d a d a k b g b d b d c d c d a k b d a j b f c e b e b d c g c f \n",
      "[start] c d c e b g b d a d f g b g b d a k i j b d a d k l c d a i h m ed c d a f ee ef b d b f c e b e a j ei ej ek a k eh el a f eg em b d a f fd fe c g a f ff fg a k e fh c f a g fi fj a f d fk [end]\n",
      "[start] c d c e b g b d a d f g b g b d a k i j b d c d a j k l m c d a i ed ee b d b f c e a f eh ei a k eg ej a f ef ek b d a f el em b g a f fd fe c f a f ff fg a f f fh a k e fi c f a g fj fk a f d fl [end]\n",
      "a h a g c g c f c g a i a g a h c e a e a g a i a k a i b f a d c d b g b f c g c f b f b d c e b f a k b e b e b g \n",
      "[start] c g c f a g d e c g c e b f c d b g a d j k b f a i i l m c g a k ed ee c f b f a i ef eg eh b d a g ei ej a e ek c e a h h el em b f a g fd fe b e b e a k fg fh b g a i ff fi fj a h f g fk [end]\n",
      "[start] c g c f a g d e c g c e b f c d b g a d j k b f c g a i l m ed b f a i i ee ef b d a k eg eh b f a g ei ej a e ek c e a h h el em b f a g fd fe b e b g a k fg fh a i g ff fi c g a h f fj fk [end]\n",
      "a f a i c e a j a f a g a j c f a f a i c e c f b d c g c d b g b e a k c e a e b e c f b g c d \n",
      "[start] c e c f c e c f b d a i f g h c g a f i j c d a j e k l b g a g m ed b e a f ee ef c e b e a e ei a k eh ej c f a j eg ek el b g a i d em fd c d a f fe ff [end]\n",
      "[start] c e c f c e c f b d a i f g h c g a f i j c d a j e k l b g a g m ed b e c e b e a e eh a k eg ei a j ee ef ej c f a f ek el b g a i d em fd c d a f fe ff [end]\n",
      "a e a h a g c d a e c f a i b d a h c e c f a j b g c e b d c g a h c f a j a j c d c f c f c e b f c g \n",
      "[start] c d c f a e e a g d f b d c e c f b g c e b d a j k l m a h i j ed c g a i h ee ef c f c d c f c f a j ei ej ek c e b f a j el em fd c g a h eh fe ff a h g eg fg a e fh [end]\n",
      "[start] c d c f a e e a g d f b d c e c f b g c e b d a j k l m a h i j ed c g a i h ee ef c f c f c f c f a j ei ej ek c e b f a j el em fd a h eg eh fe c g a h g ff fg a e fh [end]\n",
      "a k a g b g a g a h a d c f a g a j b e c g b d a e a f a e b g a g a i b e a e b d a e a f c g c e b g c e c e c f b d \n",
      "[start] b g c f b e c g b d a j f g h b g a e j b e b d a e m c g c e a f ee ef a e eg a i l ed eh b g a g ei ej a f k ek a e el a g i em a d e fd c e c e a h fe ff fg c f a g fh fi a g d fj b d a k fk fl [end]\n",
      "[start] b g c f b e c g b d a j f g h b g a e j b e b e c d a e ed a d m ee a e ef c g b e c f a g ei ej a i eg eh ek a f l el a e em a j i k fd a d e fe c e a g ff fg a g d fh c f a g fi fj b d a k fk fl [end]\n",
      "a i c d a h a e b e a f a k a f c g a k b g a d c f b g a e b e b f b f c g \n",
      "[start] c d b e a e e c g b g c f b g a d i j a k h k a f g l b e a e ed a k m ee b f a f ef eg b f a h f eh ei c g a i d ej ek [end]\n",
      "[start] c d b e a e e c g b g c f b g a d i j a k h k a f g l b e a e ed a k m ee b f a f ef eg b f a h f eh ei c g a i d ej ek [end]\n",
      "a e a d b f a f a f a d a g a d a g a g c f c e c e a e c f a k b g b d b d c d a i a k c e c d a h a e c f c d b f b f \n",
      "[start] b f c f c e a g e f c e a g g h c f a e j a d i k b g b d a k m ed a g l ee b d a d ef eg c d a f eh ei c e c d a k ek el c f a e fd c d b f a h fe ff fg b f a i em fh fi a f ej fj a d d fk a e fl [end]\n",
      "[start] b f c f c e a g e f c e a g g h c f a e j a d i k b g b d a f m ed a g l ee b d a d ef eg c d a f eh ei c e c d a k ek el c f a e fd c d a h em fe ff a f ej fg a d d fh b f a f fi fj a e fk a e fl [end]\n",
      "a e a f a d a j a h c f c g a k b f b d a k c g c f b e b e b f \n",
      "[start] c f c g b f b d a k f g a h d e h c g c f a k j k b e a j i l m b e a d ed ee b f a f ef eg a e eh [end]\n",
      "[start] c f c g b f b d a k f g a h d e h c g c f a k j k b e a j i l m b f a d ed ee b f a f ef eg a e eh [end]\n",
      "a j c e a d a d a d a k c g c d a d c g b f c f a e a e a e c f a h c e b f b f \n",
      "[start] c e c g c d a k e f c g b f a d h i a d g j c f a d k l c f a e ed a e ee a e ef a d m eg c e b f b f a h ei ej ek a j d eh el [end]\n",
      "[start] c e c g c d a k e f c g b f a d h i a d g j c f a d k l c f a e ed a e ee a d m ef c e b f b f a h eh ei ej a j d eg ek [end]\n",
      "a d a i a j b d b e b g a k c d a j a k a f a k b g b e c g c f a e c g a k b g b e c g a i a g b d a k c f c g b f a k c f b f \n",
      "[start] b d b e b g a j d e f c d b g b e a k i j c g a f k l c f a k m ed c g a e ef b g b e a k eh ei a j ee eg ej a k h ek c g a i g el em b d c f c g a k ff fg a g fe fh b f c f b f a k fk fl a i fi fj fm a d fd gd [end]\n",
      "[start] b d b e b g a j d e f c d b g b e a k i j c g a f k l c f a k m ed c g b g b e a k eg eh a f ef ei c g b d c f a k el em a g ek fd c g a i ej fe ff b f c f b f a k fi fj a i fg fh fk a k ee fl a i g h fm [end]\n",
      "a f a f a f a h c f b g a j a i b f a j c e b e b e b e b e c e b d c d b d \n",
      "[start] c f b g b f c e b e b e a j g h i b e a i f j k b e c e a j l m ed a h d e ee b d a f ef eg c d a f eh ei b d a f ej ek [end]\n",
      "[start] c f b g b f c e b e b e a j g h i b e a i f j k b e a j e l m c e a h d ed ee b d a f ef eg c d a f eh ei b d a f ej ek [end]\n",
      "a k a d c g b g a k c d a i b g a g c d b g a j a i c d c g c d b d a e c e \n",
      "[start] c g b g a d d e c d b g c d b g a g i j c d c g c d a i l m ed b d c e a e eg a j ee ef eh a i h k ei a k g ej a k f ek [end]\n",
      "[start] c g b g a d d e c d b g c d b g a g i j c d c g c d a i l m ed b d c e a e eg a j ee ef eh a i h k ei a k g ej a k f ek [end]\n",
      "a j a e a k b d a j c f b g a k b f c f a g a e b d a k a g c e b e b f b e \n",
      "[start] b d c f b g b f c f a k g h a j e f i a k d j a e k b d a e m c e b e a g ee ef b f a k eg eh a g ed ei b e a j l ej ek [end]\n",
      "[start] b d c f b g b f c f a k g h a j e f i a k d j a e k b d a e m c e b e a g ee ef b f a k eg eh a g ed ei b e a j l ej ek [end]\n",
      "a k a d c g b g a k c d a i b g a g c d b g a j a i c d c g c d b d a e c e \n",
      "[start] c g b g a d d e c d b g c d b g a g i j c d c g c d a i l m ed b d c e a e eg a j ee ef eh a i h k ei a k g ej a k f ek [end]\n",
      "[start] c g b g a d d e c d b g c d b g a g i j c d c g c d a i l m ed b d c e a e eg a j ee ef eh a i h k ei a k g ej a k f ek [end]\n",
      "a j c f a h a g b d a g a f b d a k b g b g b g c f c e a f b f a j a e b d c f a g c g c f \n",
      "[start] c f b d b d b g b g a k g h a f f i b g a g j k a g e l c f c e a h m ed ee b f b d a e eh c f c g c f a g ek el a j ei ej em a f eg fd a j d ef fe [end]\n",
      "[start] c f b d b d b g b g a k g h a f f i b g a g j k a g e l c f c e a f ed ee b f b d a e eh c f c g c f a g ek el a j ei ej em a f eg fd a h m ef fe a j d ff [end]\n",
      "a h a k c g a j a e b e a i a k b d b e a k b g a j b g b d b d b d b e c e a k a k b d b f c e \n",
      "[start] c g b e a e e b d b e a k g h b g b g b d b d a j k l m a k j ed b d a i i ee ef b e a j f eg eh a k d ei c e b d b f a k el em c e a k fd fe a h ej ek ff [end]\n",
      "[start] c g b e a e e b d b e a k g h b g b g b d b d a j k l m a k j ed b e a i i ee ef b e a j f eg eh a k d ei c e b d b f a k el em a k ek fd c e a h ej fe ff [end]\n",
      "a e a g a e a e a h b g a k a g a k b f a e b e b g b d b f a e c d \n",
      "[start] b g b f b e a e f a k e g b g a g h i b d a k j k b f a h d l m a e ed a e ee c d a e eg a g ef eh a e ei [end]\n",
      "[start] b g b f b e a e f a k e g b g a e i a k h j b d a h d k l a e m a e ed c f a e ef a g ee eg a e eh [end]\n",
      "a j b d a e a k b e a d a i c g a j b f c d a k a g c g a i b d b f a k c d a g b f b e b d c g b g b f \n",
      "[start] b d b e c g b f c d c g b d b f c d b f b e a g m ed a k l ee a i j k ef a g i eg b d a k eh ei a j g h ej c g a i f ek el b g a d em fd a k e fe a e ff b f a j d fg fh [end]\n",
      "[start] b d b e c g b f c d c g b d b f c d a i j k l a k i m a g h ed b f b e a g ef eg a k ee eh b d a j g ei ej c g a d ek el a d f em a k e fd b f a d fe ff b f a j d fg fh [end]\n",
      "a j a e c f a d b d b f a j a k b f a g b e c g a f b f b g b f \n",
      "[start] c f a e d b d b f a d f g b f b e c g a g j k a k i l b f b g a f ed ee b f a j m ef eg a j e h eh [end]\n",
      "[start] c f a e d b d b f a d f g b f b e c g a g j k a k i l b f b g a f ed ee b f a j m ef eg a j e h eh [end]\n",
      "a i b e a g a e c g c e a d c g a h c f a k b d c g a j b f b d c e \n",
      "[start] b e c g a e e c e a g f g c g c f b d c g a k k l b f b d c e a j ed ee ef a h j m eg a d i eh a i d h ei [end]\n",
      "[start] b e c g a e e c e a g f g c g c f b d c g a k k l b f b d c e a j ed ee ef a h j m eg a d i eh a i d h ei [end]\n",
      "a h a k a d a f c e a e a h c g b d b d a g c g a e c d c e a j c g a h b e a g c f c e b g b g a i b d b g b e \n",
      "[start] c e c g b d b d a h e f g a e h a f d i c g c d a e l a g k m a d j ed c e a k ee ef c g b e c f c e a g ej ek b g a h ei el em b g a j eh fd fe b d b g b e a i fg fh fi a h eg ff fj [end]\n",
      "[start] c e c g b d b d a h e f g a e h a f d i c g c d a e l a g k m a d j ed c e a k ee ef c g b e c f c e a g ej ek b g a h ei el em a j eg eh fd b d b g b e b e a i fg fh fi a h fe ff fj [end]\n",
      "a g a f c d a e a j a e a k b g b f a d b g a g c f c f b e a e a d b e b g \n",
      "[start] c d b g b f a k e f a e g b g c f c f a g j k a d i l b e a j h m ed a e ee a f d ef b e b g a d eh ei a e ej a g eg ek [end]\n",
      "[start] c d b g b f a k e f a e g b g c f c f a g j k a d i l b e a j h m ed a e ee a f d ef b e b g a d eh ei a g eg ej [end]\n",
      "a f c g a f c g a i b g c g a i c d a g c e c f a h b f b d b g \n",
      "[start] c g c g b g c g c d c e c f a g i j b f b d b g a h l m ed a i h k ee a i f g ef a f e eg a f d eh [end]\n",
      "[start] c g c g b g c g c d c e c f a g i j b f b d b g a h l m ed a i e f ee a i f g ef a f e eg a f d eh [end]\n",
      "a h c e c d a f a k a d a e a g a k b g a e b e b g c d a d c d b d c e \n",
      "[start] c e c d b g b e a e g a k f h b g a g i j a e k c d a d l m c d b d a d ee ef a k ed eg c e a f eh ei a h d e ej [end]\n",
      "[start] c e c d b g b e a e g a k f h b g a g i j a e k c d a d l m c d b d a d ee ef a k ed eg c e a f eh ei a h d e ej [end]\n",
      "a h c e b f a j b g a j b g a e a g b e a j b g c g c e c f c g \n",
      "[start] c e b f b g b g b e b g c g c e a j i j k a g h l a e m c f a j g ed ee c g a j f ef eg a h d e eh [end]\n",
      "[start] c e b f b g b g b e b g c g c e a j i j k a g h l a e m c f a j g ed ee c g a j f ef eg a h d e eh [end]\n",
      "a h a f a h b d a g b g b e a f b f a j b e b e c e c e c d b g \n",
      "[start] b d b g b e a g e f b f b e b e c e a j i j k a f h l a h d g m c e a f ed ee c d b g a h ef eg eh [end]\n",
      "[start] b d b g b e a g e f b f b e b e c e a j i j k a f h l a h d g m c e a f ed ee c d b g a h ef eg eh [end]\n",
      "a e a k a h a i b g a g b d b d a k a i a k c e c e c e b f a k c g c f c e a g b f c d b f \n",
      "[start] b g b d b d a g e f c e c e a k h i c e b f a i j k l c g c f a k ed ee a k m ef a i d g eg c e b f c d a g ej ek a h eh ei el b f a k em fd a e fe [end]\n",
      "[start] b g b d b d a g e f c e c e a k h i c e b f a i j k l c g c f a k ed ee a k m ef a i d g eg c e b f c d a g ej ek a h eh ei el b f a k em fd a e fe [end]\n",
      "a k a h b g a k a e a j c g c e a d c e a f c f b g b f b f a e c f \n",
      "[start] b g c g c e c e c f b g a f h i a d g j a j e f k a e l b f a k m ed b f a h d ee ef c f a e eh a k eg ei [end]\n",
      "[start] b g c g c e c e c f b g a f h i a d g j a j e f k a e l b f a k m ed b f a h d ee ef c f a e eh a k eg ei [end]\n",
      "a d a k c g a i a k a k a e b g a j b e c f a g b f a h b g c e b d a j c g b f c e b d b f b d \n",
      "[start] c g b g a e e b e c f b f b g c e b d a h j k l a g i m a j g h ed a k f ee c g b f c e a j eg eh ei a k ef ej b d b f a i ek el em a k d fd b d a d fe ff [end]\n",
      "[start] c g b g a e e b e c f b f b g c e b d a h j k l a g i m a j g h ed a k f ee c g b f c e a j eg eh ei a k ef ej b d b f a i ek el em a k d fd b d a d fe ff [end]\n",
      "a f a j b g a f a d c d c d a i a g c d c e c d a h b d a e c g b g a k c g a f b f a j b e c e c g b e \n",
      "[start] b g c d c d a d e f c d c e a g h i c d b d c g a e m b g a h l ed ee a i j k ef a f g eg c g b f b e c e c g a j ek el em a f ej fd a k ei fe a j d eh ff b e a f fg fh [end]\n",
      "[start] b g c d c d a d e f c d c e c d a g i j b d c g a e m b g a h l ed ee c g b f b e c e c g a j ei ej ek a f eh el a k eg em a i k ef fd a f h fe a j d g ff b e a f fg fh [end]\n",
      "a j a e c f a d a f a k b g a d b e a e c e c d a j c e a k a i a d b f b e a e b g a k c d c d c g c e b d \n",
      "[start] c f a e d b g b e c e a e h a d g i a k f j c d a f k l c e b f b e a d ee ef b g a e eh c d c d a k ej ek a i eg ei el c g a k em fd c e a j ed fe ff a d m fg b d a j e fh fi [end]\n",
      "[start] c f a e d b g b e c e a e h a d g i a k f j c d a f k l c e b f b e a d ee ef b e c d c d a k ei ej a i eg eh ek c g a j ed el em c e a k fd fe a d m ff b d a j e fg fh [end]\n",
      "a g a f c f a h c g b f a e c g a g a e a d b f a k c d a f a e c g c e c d \n",
      "[start] c f c g b f c g a e g a h e f h a f d i b f c d c g a e m c e a f ed ee a k l ef a d k eg a e eh c d a g ei ej a g j ek [end]\n",
      "[start] c f c g b f c g a e g a h e f h a f d i b f c d c g a e m c e a f ed ee a k l ef a d k eg c d a g eh ei a g j ej [end]\n",
      "0.7717917675544794\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 100\n",
    "\n",
    "def decode_sequence(input_sentence, model):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence])[:, :-1]\n",
    "        predictions = model(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "def transform_format(true_output_sequence, pred_sequence):\n",
    "    true_output_sequence = true_output_sequence.split()\n",
    "    pred_sequence  = pred_sequence.split()\n",
    "    true_output_sequence.remove('[start]')\n",
    "    true_output_sequence.remove('[end]')\n",
    "    pred_sequence.remove('[start]')\n",
    "    pred_sequence.remove('[end]')\n",
    "    return true_output_sequence, pred_sequence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "test_spa_texts = [pair[1] for pair in test_pairs]\n",
    "\n",
    "count_total = 0\n",
    "count_match =0 \n",
    "for i in np.random.randint(len(test_eng_texts),size=50):\n",
    "    input_sequence = test_eng_texts[i]\n",
    "    true_output_sequence = test_spa_texts[i]    \n",
    "    pred_sequence  = decode_sequence(input_sequence,transformer)\n",
    "    print(input_sequence)\n",
    "    print(true_output_sequence)\n",
    "    print(pred_sequence)\n",
    "    true_output_sequence, pred_sequence=transform_format(true_output_sequence, pred_sequence)\n",
    "    count_total+=len(true_output_sequence)\n",
    "    for i, token in enumerate(pred_sequence):\n",
    "        if i<len(true_output_sequence):\n",
    "            if token == true_output_sequence[i]:\n",
    "              count_match+=1\n",
    "        else:\n",
    "            break\n",
    "  # print(count_match)\n",
    "  # print(len(true_output_sequence))\n",
    "  # print(count_match/len(true_output_sequence))\n",
    "\n",
    "print(count_match/count_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as can be seen, the accuracy is 0.77 for 50 testing text sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqQMOSvrjqc8"
   },
   "source": [
    "## 4) Reload the model and test again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "m2s11FzN0e7V"
   },
   "outputs": [],
   "source": [
    "# At loading time, register the custom objects with a `custom_object_scope`:\n",
    "saved_model = keras.models.load_model(\"artificial_text_translation.keras\", custom_objects={\n",
    "'TransformerEncoder': TransformerEncoder,\n",
    "'TransformerDecoder': TransformerDecoder,\n",
    "'PositionalEmbedding': PositionalEmbedding\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Cvrl73zJiM2"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project_2_Jungang_(Gordon)_Chen_528007929.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
